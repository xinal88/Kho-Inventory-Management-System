{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9438306,"sourceType":"datasetVersion","datasetId":5735157}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"33fe3b3b","cell_type":"markdown","source":"<img src=\"https://devra.ai/analyst/notebook/2108/image.jpg\" style=\"width: 100%; height: auto;\" />","metadata":{}},{"id":"dfca64ba","cell_type":"markdown","source":"<div style=\"text-align:center; border-radius:15px; padding:15px; color:white; margin:0; font-family: 'Orbitron', sans-serif; background: #2E0249; background: #11001C; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.3); overflow:hidden; margin-bottom: 1em;\">    <div style=\"font-size:150%; color:#FEE100\"><b>SuperMarket Sales Analysis Notebook</b></div>    <div>This notebook was created with the help of <a href=\"https://devra.ai/ref/kaggle\" style=\"color:#6666FF\">Devra AI</a></div></div>","metadata":{}},{"id":"60713f02","cell_type":"markdown","source":"## Table of Contents\n- [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n- [Data Cleaning and Preprocessing](#Data-Cleaning-and-Preprocessing)\n- [Prediction Modeling](#Prediction-Modeling)\n- [Conclusion and Future Work](#Conclusion-and-Future-Work)","metadata":{}},{"id":"ed02f81f","cell_type":"code","source":"# Importing necessary libraries and suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use Agg backend to avoid GUI issues\nimport matplotlib.pyplot as plt\nplt.switch_backend('Agg')  # In case only pyplot is used\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline","metadata":{},"outputs":[],"execution_count":null},{"id":"5812fb46","cell_type":"code","source":"# Load the Supermarket Sales dataset\ndf = pd.read_csv('/kaggle/input/supermarket-sales/SuperMarket Analysis.csv', encoding='UTF-8-SIG', delimiter=',')\n\n# A quick look at the first few rows of the dataset\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"id":"32040493","cell_type":"markdown","source":"## Data Loading and Preprocessing","metadata":{}},{"id":"5e6e7ceb","cell_type":"code","source":"# Display basic information about the dataset\nprint('Shape of the dataframe:', df.shape)\nprint('\\nData Types:')\nprint(df.dtypes)\n\n# Overview of missing values if any\nprint('\\nMissing values in each column:')\nprint(df.isnull().sum())","metadata":{},"outputs":[],"execution_count":null},{"id":"354978ca","cell_type":"markdown","source":"## Exploratory Data Analysis\n\nIt is intriguing to note that sales data from a supermarket can be surprisingly rich in insights. Here, we explore relationships among numerical features and distributions of categorical variables. If you find these visualizations and analyses useful, please upvote.","metadata":{}},{"id":"8ba57d89","cell_type":"code","source":"# Plot a count plot for the Payment column\nplt.figure(figsize=(8, 4))\nsns.countplot(data=df, x='Payment', palette='pastel')\nplt.title('Distribution of Payment Methods')\nplt.xlabel('Payment Method')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n\n# Plotting histogram for the Rating column\nplt.figure(figsize=(8, 4))\nsns.histplot(df['Rating'], kde=True, color='teal')\nplt.title('Distribution of Customer Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n# Preparing a correlation heatmap. We first reduce the dataframe to numeric columns.\nnumeric_df = df.select_dtypes(include=[np.number])\n\nif numeric_df.shape[1] >= 4:\n    plt.figure(figsize=(10, 8))\n    corr = numeric_df.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title('Correlation Heatmap for Numeric Features')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Not enough numeric columns for a correlation heatmap.')","metadata":{},"outputs":[],"execution_count":null},{"id":"69918f11","cell_type":"code","source":"# Creating a pair plot to visualize relationships between selected numeric features\nselected_cols = ['Unit price', 'Quantity', 'Tax 5%', 'Sales', 'cogs', 'gross income', 'Rating']\nsns.pairplot(df[selected_cols], kind='scatter', diag_kind='hist', plot_kws={'alpha':0.6})\nplt.suptitle('Pair Plot of Selected Numeric Features', y=1.02)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"8bb48bc3","cell_type":"markdown","source":"## Data Cleaning and Preprocessing\n\nA careful examination of the data attributes can help address common issues like incorrect data types and missing values. Here we transform the 'Date' column into a datetime type and ensure that the dataset is ready for further analysis.","metadata":{}},{"id":"ea2857c5","cell_type":"code","source":"# Convert 'Date' column to datetime type\ndf['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce')\n\n# If there are errors converting dates, they will appear as NaT\nif df['Date'].isnull().sum() > 0:\n    print('Some dates could not be converted. Consider checking the format or handling NaT values.')\n\n# Optionally, convert 'Time' column to datetime.time if needed\n# Here we keep it as string or you may also use pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time\n\n# Check for duplicate rows\ndupes = df.duplicated().sum()\nprint(f'Number of duplicate rows: {dupes}')\n\n# Dropping duplicate rows if any\nif dupes > 0:\n    df.drop_duplicates(inplace=True)\n    print('Duplicates have been dropped.')\n\n# Final check for missing values\nprint('\\nMissing values in each column after cleaning:')\nprint(df.isnull().sum())","metadata":{},"outputs":[],"execution_count":null},{"id":"aa4ab396","cell_type":"markdown","source":"## Prediction Modeling\n\nInspired by the richness of this dataset, we explore building a predictor. In this example we attempt to predict the Payment method used by customers based on a variety of features. The process involves encoding categorical variables, splitting the data, training a Random Forest classifier, and evaluating the results with an accuracy score and a confusion matrix. The approach, although straightforward, sets a solid base for further refinement.","metadata":{}},{"id":"8a4b2d39","cell_type":"code","source":"# Building a predictor to classify the Payment method\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Drop columns that are less useful for prediction\n# We drop Invoice ID, Date, Time and Payment (the target) from features\nfeatures = df.drop(columns=['Invoice ID', 'Date', 'Time', 'Payment'])\ntarget = df['Payment']\n\n# Identify categorical and numerical columns\ncategorical_cols = features.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = features.select_dtypes(include=[np.number]).columns.tolist()\n\n# Preprocessing for numerical data\nnumerical_transformer = StandardScaler()\n\n# Preprocessing for categorical data\ncategorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')\n\n# Bundle preprocessing for both numeric and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define the model\nmodel = RandomForestClassifier(random_state=42, n_estimators=100)\n\n# Create and fit the pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)\n\nclf.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Prediction Accuracy: {:.2f}%'.format(accuracy * 100))\n\n# Plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix for Payment Prediction')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.tight_layout()\nplt.show()\n\n# Plot feature importances from the Random Forest\n# Note: To extract feature names after transformation, we need to process the pipeline\npreprocessed_features = preprocessor.fit_transform(X_train)\n\n# Get numeric feature names\nnum_features = numerical_cols\n\n# Get categorical feature names from OneHotEncoder\ncat_features = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n\nall_features = num_features + cat_features\n\nimportances = model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 10))\nplt.barh(range(len(importances)), importances[indices], align='center', color='mediumseagreen')\nplt.yticks(range(len(importances)), [all_features[i] for i in indices])\nplt.xlabel('Feature Importance')\nplt.title('Permutation Importance (Approximation) of Features')\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"9f7f9970","cell_type":"markdown","source":"## Conclusion and Future Work\n\nOur investigation of the SuperMarket Sales dataset has revealed interesting relationships among numerical and categorical variables. We have also demonstrated a prediction pipeline to classify customer Payment methods based on other features. \n\nThe approach taken in this notebook makes use of robust preprocessing methods and visualizations to ensure accurate insights. One merit of this approach is its modularity in encoding, visualization, and modeling. Future work could explore deeper feature engineering (such as time-based trends and customer segmentation) and a comparison of different classification algorithms. \n\nIf you found this notebook useful, please consider upvoting it.","metadata":{}}]}